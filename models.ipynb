{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading models with Transformer Architectures"
      ],
      "metadata": {
        "id": "Dc5JgtSmSEe9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azFyz9sKR9dn"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformers library under the hood makes use of a framework for machine learning computations, which can either be PyTorch or Tensorflow. In this practical session, we'll make use of PyTorch. We'll load PyTorch using the command below; we'll equally define that PyTorch does not need to take into account gradient computations. Gradient computations are only necessary when training (or finetuning) the models. In this practical we'll only perform inference, and no training or finetuning - as training and finetuning is rather compute intensive.\n",
        "\n",
        "By disabling gradient computations when we don't need them, we'll speed up the computations."
      ],
      "metadata": {
        "id": "evWT8NGVScCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "id": "EIjzvtOiSY5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Llama-2\n",
        "\n",
        "We will use transformers in a decoder setup: the model is only able to look at the tokens that have already been generated at a certain point in time, and uses this information in order to predict the next token in the sequence. This is the setup of the infamous GPT (Generative Pretrained Transformer) models. The most recent instantiation is GPT-4o, but its parameters (a massive amount) are not publicly available. We can import the necessary modules with the following command:"
      ],
      "metadata": {
        "id": "BrgYwjtJShhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "dJLHSRz5S_00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can load the model's parameters and tokenizer using the following commands:"
      ],
      "metadata": {
        "id": "WrLMvwSxTD4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "c_gSdVy0TFsl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}